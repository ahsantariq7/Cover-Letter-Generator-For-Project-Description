{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19323770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8989/587196773.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "2024-04-29 20:27:06.266703: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-29 20:27:07.046219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-29 20:27:08.108127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to /home/ahsan-\n",
      "[nltk_data]     pmylsp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ahsan-\n",
      "[nltk_data]     pmylsp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ahsan-\n",
      "[nltk_data]     pmylsp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (if not downloaded already)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db5c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"pcl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c80b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Cover Letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I need a Python developer for a long-term part...</td>\n",
       "      <td>I am writing to express my interest in the Pyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Need to write a file handling program in Pytho...</td>\n",
       "      <td>I hope this email finds you well. I am writing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need someone to help me with Agent to contro...</td>\n",
       "      <td>I am writing to express my keen interest in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are seeking a skilled Python coder to assis...</td>\n",
       "      <td>I am writing to express my interest in the Pyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have a scenario with a set of questions, you...</td>\n",
       "      <td>I am writing to express my interest in the sce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i have a python script. It is download a file ...</td>\n",
       "      <td>I am writing to express my interest in the opp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I need an individual to scrape data from ali e...</td>\n",
       "      <td>I am reaching out to express my interest in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nJoin our dynamic team as a Junior Python D...</td>\n",
       "      <td>I am excited to apply for the Junior Python De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\nAre you passionate about Python programmin...</td>\n",
       "      <td>\\n\\nI am writing to express my interest in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Project Description  \\\n",
       "0  I need a Python developer for a long-term part...   \n",
       "1  Need to write a file handling program in Pytho...   \n",
       "2  I need someone to help me with Agent to contro...   \n",
       "3  We are seeking a skilled Python coder to assis...   \n",
       "4  I have a scenario with a set of questions, you...   \n",
       "5  i have a python script. It is download a file ...   \n",
       "6  I need an individual to scrape data from ali e...   \n",
       "7  \\n\\nJoin our dynamic team as a Junior Python D...   \n",
       "8  \\n\\nAre you passionate about Python programmin...   \n",
       "\n",
       "                                        Cover Letter  \n",
       "0  I am writing to express my interest in the Pyt...  \n",
       "1  I hope this email finds you well. I am writing...  \n",
       "2  I am writing to express my keen interest in th...  \n",
       "3  I am writing to express my interest in the Pyt...  \n",
       "4  I am writing to express my interest in the sce...  \n",
       "5  I am writing to express my interest in the opp...  \n",
       "6  I am reaching out to express my interest in th...  \n",
       "7  I am excited to apply for the Junior Python De...  \n",
       "8  \\n\\nI am writing to express my interest in the...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4cce97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [word.translate(table) for word in tokens]\n",
    "    # Remove non-alphabetic tokens\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786c584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Extract project descriptions and cover letters\n",
    "project_descriptions = df['Project Description'].tolist()\n",
    "cover_letters = df['Cover Letter'].tolist()\n",
    "\n",
    "# Preprocess the project descriptions and cover letters\n",
    "preprocessed_project_descriptions = [preprocess_text(desc) for desc in project_descriptions]\n",
    "preprocessed_cover_letters = [preprocess_text(letter) for letter in cover_letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b2f2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tokenize input and output sequences\n",
    "tokenizer_desc = Tokenizer()\n",
    "tokenizer_desc.fit_on_texts(preprocessed_project_descriptions)\n",
    "tokenizer_cover = Tokenizer()\n",
    "tokenizer_cover.fit_on_texts(preprocessed_cover_letters)\n",
    "\n",
    "# Convert text sequences to integer sequences\n",
    "X = tokenizer_desc.texts_to_sequences(preprocessed_project_descriptions)\n",
    "y = tokenizer_cover.texts_to_sequences(preprocessed_cover_letters)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_seq_length = max(max(len(seq) for seq in X), max(len(seq) for seq in y))\n",
    "X = pad_sequences(X, maxlen=max_seq_length, padding='post')\n",
    "y = pad_sequences(y, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a139e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ef882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "def create_model(input_vocab_size, output_vocab_size, max_seq_length, hidden_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_vocab_size, hidden_units),\n",
    "        tf.keras.layers.LSTM(hidden_units),\n",
    "        tf.keras.layers.RepeatVector(max_seq_length),\n",
    "        tf.keras.layers.LSTM(hidden_units, return_sequences=True),\n",
    "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(output_vocab_size, activation='softmax'))\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1917639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 21:50:56.451655: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT32 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 5.9493 - val_loss: 5.8519\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - loss: 5.8715 - val_loss: 5.6485\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 5.7085 - val_loss: 5.0070\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 5.1938 - val_loss: 4.0683\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - loss: 4.4623 - val_loss: 3.8853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7a15db738490>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create model\n",
    "input_vocab_size = len(tokenizer_desc.word_index) + 1\n",
    "output_vocab_size = len(tokenizer_cover.word_index) + 1\n",
    "hidden_units = 256\n",
    "model = create_model(input_vocab_size, output_vocab_size, max_seq_length, hidden_units)\n",
    "\n",
    "# Train model\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4bc563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the trained model\n",
    "model.save('cover_letter_generator_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58734f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('cover_letter_generator_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc2c895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Generated Cover Letter:\n",
      "warm prospect fulfill opportunity capable      appropriately    efficiency  network expertise    express  journey         various strengthen response price perfectly    compression  scheduling open    innovation confident   critically  look completion sale   extract enthusiastic   desire   development     team address handling completion strong  moreover   my contributing  contact contact file duration detail reviewed desire  assignment coding scenariobased price agent    help   provided    worked  system  support   clear highquality  maintains   python      code communication contribute  detail  genuinely  some performance   file   crosschecking  align genuinely  committed   gz genuine       proficiency service    background developer    storage \n"
     ]
    }
   ],
   "source": [
    "# New project description\n",
    "new_project_description = [\"I need a python developer\"]\n",
    "\n",
    "# Preprocess the new project description\n",
    "preprocessed_new_project_description = preprocess_text(new_project_description[0])\n",
    "\n",
    "# Convert the preprocessed project description to sequences using the tokenizer\n",
    "new_project_sequence = tokenizer_desc.texts_to_sequences([preprocessed_new_project_description])\n",
    "\n",
    "# Pad the sequence to match the input sequence length used during training\n",
    "new_project_padded_sequence = pad_sequences(new_project_sequence, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "\n",
    "# Predict the cover letter for the new project description\n",
    "predicted_cover_letter_sequence = model.predict(new_project_padded_sequence)\n",
    "\n",
    "# Convert the predicted sequence back to text using the tokenizer for cover letters\n",
    "predicted_cover_letter_text = []\n",
    "for sequence in predicted_cover_letter_sequence[0]:  # Take the first sequence (as there's only 1)\n",
    "    # Sample a token based on its probability distribution\n",
    "    sampled_token_index = np.random.choice(len(sequence), p=sequence)\n",
    "    # Convert the index to its corresponding word\n",
    "    word = tokenizer_cover.index_word.get(sampled_token_index, '')\n",
    "    # Append the word to the cover letter text\n",
    "    predicted_cover_letter_text.append(word)\n",
    "\n",
    "# Join the words to form the predicted cover letter text\n",
    "predicted_cover_letter_text = ' '.join(predicted_cover_letter_text)\n",
    "\n",
    "# Print the generated cover letter\n",
    "print(\"Generated Cover Letter:\")\n",
    "print(predicted_cover_letter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc35e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ad11d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
